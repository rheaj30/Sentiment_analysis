{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e448f8d",
   "metadata": {},
   "source": [
    "# Function for cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca12f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import treebank,stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import openpyxl\n",
    "from openpyxl.utils import get_column_letter\n",
    "0\n",
    "def preprocess_tweets(text):\n",
    "\n",
    "    #CONVERT TO LOWER CASE\n",
    "    text=text.lower()\n",
    "\n",
    "    #Remving hyperlinks\n",
    "    text= re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    #TOKENIZE THE WORDS\n",
    "    words=word_tokenize(text,\"english\")\n",
    "\n",
    "    #Removing all the stop words\n",
    "    words=[w for w in words if w not in stopwords.words('english')]\n",
    "\n",
    "    stemmer=PorterStemmer()\n",
    "    words=[stemmer.stem(w) for w in words]\n",
    "    #Lemmatizing the words- Extracting the root word\n",
    "    #Creating instance for WordNetLemmatizer in nltk\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    words =[lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    #REMOVE ALL THE PUNCTUATIONS AND NUMERIC VALUES\n",
    "    cleaned_words=[w for w in words if w.isalpha()]\n",
    "    # print(cleaned_words)\n",
    "    # print(len(cleaned_words))\n",
    "\n",
    "    #Assiging tags\n",
    "    pos_tags=nltk.pos_tag(cleaned_words)\n",
    "    # print(pos_tags)\n",
    "\n",
    "    cleaned_text=\" \".join(cleaned_words)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83456d3d",
   "metadata": {},
   "source": [
    "# Function to create the excel file containing the cleaned tweets of 2 timeframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df4b2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_tweets(filename):\n",
    "    # open the input Excel file \n",
    "    workbook = openpyxl.load_workbook(filename)\n",
    "\n",
    "    # create a new workbook to write the output\n",
    "    output_workbook = openpyxl.Workbook()\n",
    "    output_workbook.remove(output_workbook.active)\n",
    "\n",
    "    # process each sheet in the input file and write to a new sheet in the output file\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        worksheet = workbook[sheet_name]\n",
    "        output_worksheet = output_workbook.create_sheet(title=f'cleaned_{sheet_name}')\n",
    "\n",
    "        # write the header row\n",
    "        header = ['screen_name', 'cleaned_text']\n",
    "        for col_num, column_title in enumerate(header, 1):\n",
    "            column_letter = get_column_letter(col_num)\n",
    "            output_worksheet[f\"{column_letter}1\"] = column_title\n",
    "\n",
    "        # iterate over the rows in the original sheet\n",
    "        for row_num in range(2, worksheet.max_row + 1):\n",
    "            screen_name = worksheet.cell(row=row_num, column=4).value\n",
    "            text = worksheet.cell(row=row_num, column=2).value\n",
    "            cleaned_text = preprocess_tweets(text)\n",
    "\n",
    "            # write the data to the output sheet\n",
    "            output_worksheet.cell(row=row_num, column=1, value=screen_name)\n",
    "            output_worksheet.cell(row=row_num, column=2, value=cleaned_text)\n",
    "    \n",
    "    filename = filename.split('/')[-1] \n",
    "    # save the output Excel file\n",
    "    output_workbook.save(f'C:/Users/ektag/Minor2/sentiment_analysis/cleaned_filtered_and_sorted_tweets/cleaned_{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "015115e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory=\"C:/Users/ektag/Minor2/sentiment_analysis/filtered_and_sorted_tweets/\"\n",
    "for filename in os.listdir(directory):\n",
    "    f=os.path.join(directory,filename)\n",
    "    if os.path.isfile(f) and filename.endswith('.xlsx'):\n",
    "        cleaning_tweets(directory+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c51e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
